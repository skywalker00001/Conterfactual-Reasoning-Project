{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_cleaning_6.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0cGyZWlCpsrJ1fiCoPqNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker00001/Conterfactual-Reasoning-Project/blob/main/data_cleaning_6_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# two stages"
      ],
      "metadata": {
        "id": "_eidGXfiA4DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to find out the different or changed blocks in sentence b compared to the sentence a (base)"
      ],
      "metadata": {
        "id": "Q2aKroRZLmfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = 'drive/MyDrive/LM/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4QazUl7U6UF",
        "outputId": "c75468fd-c229-42f0-cd8a-e0331adff8ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentencepiece\n",
        "#!pip install transformers -q"
      ],
      "metadata": {
        "id": "ZghQ4Tb8kr3S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import difflib\n",
        "import nltk\n",
        "import copy\n",
        "import regex as re\n",
        "import sys\n",
        "import torch\n",
        "sys.path.append('/content/drive/MyDrive/LM/')\n",
        "from global_param import MyConfig\n",
        "from tqdm import tqdm\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "detokenizer = TreebankWordDetokenizer()\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "KwAz8ZN7Wgb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e597b320-7244-4b86-db5f-80fd607737ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myconfig = MyConfig()"
      ],
      "metadata": {
        "id": "0XCj427dklXZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRETRAINED_MODEL_NAME = myconfig.PRETRAINED_MODEL_NAME\n",
        "# tokenzier for encoding the text\n",
        "#t5_tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "id": "_tPukXF-WWkp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.word_tokenize() as tokenizer\n",
        "def words_origin2backbone(origin, edited):\n",
        "    #origin_list = origin.split(\" \")\n",
        "    origin_list = nltk.word_tokenize(origin)\n",
        "    #print(origin_list)\n",
        "    #edited_list = edited.split(\" \")\n",
        "    edited_list = nltk.word_tokenize(edited)\n",
        "    s = difflib.SequenceMatcher(None, origin_list, edited_list)\n",
        "    cgs, answers = [], []\n",
        "    counter = 0\n",
        "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
        "        if (tag == 'equal'):\n",
        "            cgs.extend(edited_list[j1: j2])\n",
        "            #cgs.append(f'{i1}-{i2}:\\\"{(\" \").join(edited_list[j1:j2])}\\\".')\n",
        "            #cgs.append(f'{i1}<extra_id_0>{i2}<extra_id_1>{(\" \").join(edited_list[j1:j2])}<extra_id_2>')\n",
        "        elif (tag == 'insert' or tag == 'replace'): \n",
        "            cgs.append(f'<extra_id_{counter}>')\n",
        "            answers.append(f'<extra_id_{counter}>')\n",
        "            counter += 1\n",
        "            answers.append(detokenizer.detokenize(edited_list[j1: j2]).replace(' , ',',').replace(' .','.') \\\n",
        "                           .replace(' !','!').replace(' ?','?').replace(' : ',': ').replace(' \\'', '\\'')  )\n",
        "    #print(cgs)\n",
        "    conclusion = detokenizer.detokenize(cgs)\n",
        "    conclusion = conclusion.replace(' , ',',').replace(' .','.').replace(' !','!').replace(' ?','?').replace(' : ',': ').replace(' \\'', '\\'')   \n",
        "    answers = (\"\").join(answers)\n",
        "    return conclusion, answers"
      ],
      "metadata": {
        "id": "gGBO45IIGpMb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparation"
      ],
      "metadata": {
        "id": "H0_o1eQTgC08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training df\n",
        "small_path = root + '/TimeTravel/train_supervised_small.json'\n",
        "small_df = pd.read_json(small_path, lines=True)\n",
        "small_df.head()\n",
        "print(len(small_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM7Mi7sKgG7_",
        "outputId": "e05eb0a9-df67-4368-c686-5a0f75dc3853"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning, remove the examples with the same original_ending and edited_ending\n",
        "\n",
        "# comb_small_text=[]\n",
        "\n",
        "# for i in range(len(small_df)):\n",
        "\n",
        "#   comb_small_text.append(small_df.loc[i, 'edited_ending'][0] +\" \"+ small_df.loc[i, 'edited_ending'][1] +\" \"+ \\\n",
        "#                 small_df.loc[i, 'edited_ending'][2])\n",
        "# small_df.insert(6, \"ground_truth\", comb_small_text)\n",
        "# small_cleaned_df = small_df[small_df.ground_truth != small_df.original_ending]\n",
        "# small_cleaned_df = small_cleaned_df.reset_index() \n",
        "# print(len(small_cleaned_df))"
      ],
      "metadata": {
        "id": "Makx0UpcC510"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data cleaning, remove the examples with the same original_ending and edited_ending\n",
        "\n",
        "# comb_small_text=[]\n",
        "\n",
        "# for i in range(len(small_df)):\n",
        "#     edited = \"\"\n",
        "#     for j in range(len(small_df.loc[i, 'edited_ending'])):\n",
        "#         edited += small_df.loc[i, 'edited_ending'][j]\n",
        "#         if (edited[-1] != \".\"):\n",
        "#             edited += \".\"\n",
        "#         if (j != len(small_df.loc[i, 'edited_ending']) - 1):\n",
        "#             edited += \" \"\n",
        "        \n",
        "#     comb_small_text.append(edited)\n",
        "#     # comb_small_text.append(small_df.loc[i, 'edited_ending'][0] +\" \"+ small_df.loc[i, 'edited_ending'][1] +\" \"+ \\\n",
        "#     #               small_df.loc[i, 'edited_ending'][2])\n",
        "# small_df.insert(6, \"ground_truth\", comb_small_text)\n",
        "# small_cleaned_df = small_df[small_df.ground_truth != small_df.original_ending]\n",
        "# small_cleaned_df = small_cleaned_df.reset_index() \n",
        "# print(len(small_cleaned_df))"
      ],
      "metadata": {
        "id": "wlHNWbSSgG-S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input1: source, output1: target, \n",
        "premise, initial, counterfactual, original_ending, edited_ending, input1, output1, output2 = [], [], [], [], [], [], [], []\n",
        "bar = tqdm(range(len(small_df)))\n",
        "for i in bar:\n",
        "\n",
        "    premise.append(small_df.loc[i, 'premise'])\n",
        "    initial.append(small_df.loc[i, 'initial'])\n",
        "    counterfactual.append(small_df.loc[i, 'counterfactual'])\n",
        "    original_ending.append(small_df.loc[i, 'original_ending'])\n",
        "    edited = \"\"\n",
        "    for j in range(len(small_df.loc[i, 'edited_ending'])):\n",
        "        text = small_df.loc[i, 'edited_ending'][j]\n",
        "        if (text[-1] == \",\"):\n",
        "            edited += text[0:-1] + \".\"\n",
        "        else:\n",
        "            edited += text\n",
        "        #edited += small_df.loc[i, 'edited_ending'][j]\n",
        "        #if (edited[-1] != \".\" and edited[-1] != \"!\" and edited[-1] != \"?\"):\n",
        "        #    edited += \".\"\n",
        "        if (edited[-1] != \".\" and edited[-1] != \"!\" and edited[-1] != \"?\"):\n",
        "            edited += \".\"\n",
        "        if (j != len(small_df.loc[i, 'edited_ending']) - 1):\n",
        "            edited += \" \"\n",
        "    edited_ending.append(edited)\n",
        "\n",
        "    input1.append(\"premise: \" + small_df.loc[i, 'premise']  + \\\n",
        "                  \" initial: \" + small_df.loc[i, 'initial']  + \\\n",
        "                  \" original_ending: \" + small_df.loc[i, 'original_ending'] + \\\n",
        "                  \" counterfactual: \" + small_df.loc[i, 'counterfactual'] \n",
        "                  )\n",
        "    \n",
        "    conclusion, answers = words_origin2backbone(small_df.loc[i, 'original_ending'], edited)\n",
        "    output1.append(conclusion)\n",
        "    output2.append(answers)"
      ],
      "metadata": {
        "id": "oZU2FDtwgHAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa686bb4-3b12-4c12-8846-595884aa3b22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16752/16752 [00:34<00:00, 482.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#premise, initial, counterfactual, original_ending, edited_ending, input1, output1, output2\n",
        "uncleaned_train_df = pd.DataFrame({'premise': premise, 'initial': initial, 'counterfactual': counterfactual, 'original_ending': original_ending, \\\n",
        "                         'edited_ending': edited_ending, 'input1': input1, 'output1': output1, 'output2': output2}) \n",
        "uncleaned_train_df.head()\n",
        "print(len(uncleaned_train_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S08NkWiUNH5",
        "outputId": "bafdb200-3100-406f-eb74-1f3cf6bb917e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning, remove the examples with the same original_ending and edited_ending\n",
        "\n",
        "#train_df = uncleaned_train_df[uncleaned_train_df.output2 != uncleaned_train_df.original_ending]\n",
        "train_df = uncleaned_train_df[uncleaned_train_df.output2 != \"\"]\n",
        "train_df = train_df.reset_index(drop=True) \n",
        "print(len(train_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erniQrN8cm3D",
        "outputId": "7c5d9448-bbb4-4edc-b872-02795a58d69b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df.to_excel(root + 'TimeTravel/' + 'output' + model_version + '.xlsx')\n",
        "train_df.to_excel(root + 'TimeTravel/' + 'cleaned_small_2.0.xlsx', )"
      ],
      "metadata": {
        "id": "l7kJnlQQUafL"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}